# KnowBase RAG/AI Search Configuration
# Use this for conversational AI search with retrieval augmented generation

config_name: "rag_only"
description: "RAG/AI search with LLM integration (requires indexed documents)"
created_at: "2025-12-04"

preprocessing:
  chunk_size: 512
  chunk_overlap: 50
  min_chunk_size: 50
  remove_html: true
  normalize_whitespace: true
  remove_special_chars: false
  lowercase: false
  language: "en"

embedding:
  model_name: "BAAI/bge-large-en-v1.5"
  device: "auto"
  batch_size: 32
  cache_dir: "~/.cache/huggingface"
  model_cache_enabled: true
  precision: "fp32"

vector_store:
  db_path: "./data/vector_db"
  collection_name: "documents"
  distance_metric: "cosine"
  persist_directory: true

# Retrieval for RAG context
retrieval:
  top_k: 10         # More context for RAG
  similarity_threshold: 0.2
  rerank_enabled: true
  rerank_model: "ms-marco-MiniLM-L-12-v2"
  filter_by_metadata: {}

# RAG configuration - ENABLED
ai_search:
  enabled: true     # ← ENABLED
  llm_provider: "openai"
  llm_model: "gpt-4-mini"
  llm_temperature: 0.7
  llm_max_tokens: 2000
  llm_api_key: null  # Will be loaded from OPENAI_API_KEY env var
  query_analyzer_enabled: true
  query_clarity_threshold: 0.85
  query_rewriter_enabled: true
  clarification_enabled: true
  conversation_window: 10
  show_thinking: true

clustering:
  enabled: false
  min_cluster_size: 5
  min_samples: 5
  clustering_metric: "cosine"
  use_umap: true
  umap_n_neighbors: 15
  umap_min_dist: 0.1
  umap_n_components: 3

# KEY DIFFERENCE: Only run AI search (retrieval happens internally)
pipeline:
  run_preprocessing: false
  run_embedding: false
  run_indexing: false
  run_retrieval: false
  run_ai_search: true   # ← RUN ONLY THIS
  run_clustering: false
  skip_existing: false
  save_intermediate: false
  parallel_processing: true
  num_workers: -1
  verbose: false
  log_level: "INFO"
